{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_preprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sung429/DeepLearning/blob/master/Text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvOoiPW6cU0m",
        "colab_type": "text"
      },
      "source": [
        "## 단어 수준의 원-핫 인코딩하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxRe9D5ScaKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TslcRRicdHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = ['The cat saton the mat.', 'The dog ate my homework.'] #각 원소가 하나의 샘플 즉, 문장 하나하나를 말함"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8GIigrdcjfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_index = {} # 데이터에 있는 모든 토큰의 인덱스를 구축하기 위해 선언"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa-cfQbFc3bc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "46f8f00e-6466-47be-c30f-a2ec09277e97"
      },
      "source": [
        "for sample in samples:\n",
        "  for word in sample.split(): #샘플을 나눔(실전에서는 구두점과 특수문자도 사용)\n",
        "    if word not in token_index:\n",
        "      token_index[word] = len(token_index) + 1 # 단어마다 고유한 인덱스를 할당 (0은 사용하지 않음)\n",
        "\n",
        "max_length = 10 #샘플을 벡터로 변환합니다. 각 샘플에서 max_length까지 단어만 사용합니다.\n",
        "\n",
        "results = np.zeros(shape=(len(samples), max_length, max(token_index.values()) + 1)) # 결과를 저장할 배열\n",
        "print(results)\n",
        "print(results.shape)\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = token_index.get(word)\n",
        "    results[i, j, index] = 1."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "(2, 10, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmBhuFymemqQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "132a88be-4e51-4614-961c-b2db2dc933b4"
      },
      "source": [
        "results"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw-ulySve1ld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "2980753e-4239-4e4c-bde4-e22273c176ab"
      },
      "source": [
        "token_index"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'The': 1,\n",
              " 'ate': 7,\n",
              " 'cat': 2,\n",
              " 'dog': 6,\n",
              " 'homework.': 9,\n",
              " 'mat.': 5,\n",
              " 'my': 8,\n",
              " 'saton': 3,\n",
              " 'the': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j6FyLGae2S4",
        "colab_type": "text"
      },
      "source": [
        "## 문자 수준 원-핫 인코딩하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQX7G_S_fQQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOvRYfQ-fRkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = ['The cat saton the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable # 출력가능한 모든 아스키(ASCII)  문자\n",
        "token_index= dict(zip(characters, range(1, len(characters) + 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw5PkfhifWb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, character in enumerate(sample):\n",
        "    index = token_index.get(character)\n",
        "    results[i, j, index] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6z6G8qbf_cO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "bcb504ea-9a9a-4d96-c4e8-2f72d226f6f8"
      },
      "source": [
        "results"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOBlFigwgBRN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "14b2526f-f9f4-47fd-d99d-6c6058d7d999"
      },
      "source": [
        "results.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 50, 101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0CUiCtxgCvn",
        "colab_type": "text"
      },
      "source": [
        "#### 케라스에는 원본 텍스트 데이터를 단어 또는 문자 수준의 원-핫 인코딩으로 변환해 주는 유틸리티가 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFzAQNEfg6IO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "a3f62a17-433c-456c-9ab9-d981ed7a3a15"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmdLvFTyhB1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = ['The cat saton the mat.', 'The dog ate my homework.'] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovnu93PbhO0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=1000) # 가장 빈도가 높은 1,000개의 단어만 선택하도록 Tokenizer객체를 생성\n",
        "tokenizer.fit_on_texts(samples) # 단어 인덱스를 구축"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvVGJLhJhdyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(samples) # 문자열을 정수 인덱스의 리스트로 변환"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g46oawOthnZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary') #원-핫 이진벡터로 표현"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In83F5ahh4hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = tokenizer.word_index # 계산된 단어 인덱스를 구합니다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWO73aJ2iAcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "9dc6f9bd-0c56-4151-c357-12b019029d5e"
      },
      "source": [
        "word_index"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ate': 6,\n",
              " 'cat': 2,\n",
              " 'dog': 5,\n",
              " 'homework': 8,\n",
              " 'mat': 4,\n",
              " 'my': 7,\n",
              " 'saton': 3,\n",
              " 'the': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5YKqvFeiCMw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ce557a8d-b6f5-4b26-87d2-1947d177fa59"
      },
      "source": [
        "one_hot_results"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjxTp9uaiEn7",
        "colab_type": "text"
      },
      "source": [
        "#### 원-핫 해싱(one-hot hashing)기법\n",
        "- 어휘 사전에 있는 고유한 토큰의 수가 너무 커서 모두 다루기 어려울 때 사용함.\n",
        "- 각 단어에 명시적으로 인데스를 할당하고 이 인덱스를 딕셔너리에 저장하는 대신에 단어를 해싱하여 고정된 크기의 벡터로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiljY0wnjE98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = ['The cat saton the mat.', 'The dog ate my homework.'] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8TFEO3ukEOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dimensionality = 1000 # 단어를 크기가 1,000인 벡터로 저장, 1,000개 이상의 단어가 있다면 해싱 충돌이 늘어나고 인코딩의 정확도가 감소할 것임\n",
        "max_length = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RUOMZsukQZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = abs(hash(word)) % dimensionality # 단어를 해싱하여 0과 1,000 사이의 랜덤한 정수인덱스로 변환합니다.\n",
        "    results[i, j, index] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P4-GQCtkzGQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "f0820ce3-d74d-442e-b931-1a974aa17e51"
      },
      "source": [
        "results"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}